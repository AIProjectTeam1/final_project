# DPO-UA Framework

![DPO-UA Framework](images/framework.png)

## ðŸš€ How to Run

### 1. Run a Single Training Experiment

Use `multi_prompt_train.py` to run a single training job with your desired hyperparameters:

```bash
python model/multi_prompt_train.py \
  --prompt_version v2_explicit_guidelines_ko_improved \
  --learning_rate 0.0004869 \
  --num_train_epochs 5 \
  --batch_size 2 \
  --gradient_accumulation_steps 4 \
  --lora_r 16
```

#### Available Arguments

| Argument                     | Description                                               | Example Values                        |
|-----------------------------|-----------------------------------------------------------|---------------------------------------|
| `--prompt_version`          | Prompt template version to use                            | `v2_minimal`, `v2_all_eval`, etc.     |
| `--learning_rate`           | Learning rate for training                                | `1e-5`, `5e-5`, `1e-4`, etc.          |
| `--num_train_epochs`        | Number of training epochs                                 | `3`, `5`, etc.                        |
| `--batch_size`              | Batch size per GPU                                        | `2`                                   |
| `--gradient_accumulation_steps` | Number of steps for gradient accumulation             | `4`, `8`, `16`                        |
| `--lora_r`                  | Rank parameter for LoRA fine-tuning                       | `4`, `8`, `16`                        |

### 2. Run a Sweep with W&B

#### Setup

- Make sure you have a [Weights & Biases](https://wandb.ai/) account.
- Login via CLI:

```bash
wandb login
```

#### Sweep Configuration (`model/sweep.yaml`)

Example using Bayesian optimization:

```yaml
method: bayes
metric:
  name: eval_loss
  goal: minimize

parameters:
  prompt_version:
    values: ["v2_all_eval_formatted", "v2_minimal", "v2_explicit_guidelines_ko_improved"]
  learning_rate:
    min: 0.00001
    max: 0.0005
  num_train_epochs:
    values: [3, 5]
  batch_size:
    value: 2
  gradient_accumulation_steps:
    values: [4, 8, 16]
  lora_r:
    values: [4, 8, 16]
```

#### Launch the Sweep

Instead of using the default `wandb agent`, launch the sweep using your own script:

```bash
python model/sweep_runner.py
```

The `sweep_runner.py` script internally registers the sweep and uses `wandb.agent` to execute training using configurations defined in `sweep.yaml`.

## Experimental Results

#### Zero-shot vs. Few-shot Prompting
Below are example feedback excerpts generated from different prompting configurations. These are shortened samples of actual self-introduction evaluations.

##### Reference
> ìžê¸°ì†Œê°œì„œëŠ” êµ¬ì¡°ê°€ ìž˜ ìž¡í˜€ ìžˆê³ , ë…¼ë¦¬ì ìœ¼ë¡œ ìž‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì§€ì›ìžëŠ” ë³¸ì¸ì˜ ê²½ë ¥ê³¼â€¦

##### Zero-shot
> í•˜ëŠ˜ë¹›ë‹˜ì˜ ìžê¸°ì†Œê°œì„œëŠ” ê¸°ìˆ ì  ì—­ëŸ‰ê³¼ ì—´ì •, ê·¸ë¦¬ê³  ì‚¬ìš©ìž ì¤‘ì‹¬ì˜ ì ‘ê·¼ë²•ì„ ìž˜ ë³´ì—¬ì£¼ëŠ” í›Œë¥­í•œ ì˜ˆì‹œìž…ë‹ˆë‹¤. ë‹¤ìŒì€ í”¼ë“œë°±ê³¼ ê°œì„  ì œì•ˆìž…ë‹ˆë‹¤:
>
> **ê¸ì •ì ì¸ ì :**  
> **ê¸°ìˆ ì  ì—­ëŸ‰ ê°•ì¡°**: â€¦

##### One-shot
> í•˜ëŠ˜ë¹›ì˜ ìžê¸°ì†Œê°œì„œëŠ” ì „ë°˜ì ìœ¼ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ê¸°ìˆ ì  ì—­ëŸ‰ì„ ìž˜ ë³´ì—¬ì£¼ê³  ìžˆì–´ ê¸ì •ì ì¸ ì¸ìƒì„ ì¤ë‹ˆë‹¤. ê° í•­ëª©ë³„ë¡œ ëª‡ ê°€ì§€ ê°œì„  ì‚¬í•­ê³¼ ì¶”ê°€ì ì¸ í”¼ë“œë°±ì„ ì œì•ˆí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤:
>
> **#1) ë§ˆìŒì„ ë‹´ì€ ìžê¸°ì†Œê°œ: ë” ë‚˜ì€ ì‚¬ìš©ìž ê²½í—˜ì„ ìœ„í•´**  
> **ê¸ì •ì ì¸ ì :** â€¦

##### Two-shot
> í•˜ëŠ˜ë¹› ì§€ì›ìžì˜ ìžê¸°ì†Œê°œì„œëŠ” ì „ë°˜ì ìœ¼ë¡œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ê²½í—˜ê³¼ ê¸°ìˆ ì  ì—­ëŸ‰ì„ ìž˜ ë³´ì—¬ì£¼ê³  ìžˆìŠµë‹ˆë‹¤. ê° ì„¹ì…˜ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ê°œì„  ì‚¬í•­ì„ ì œì•ˆë“œë¦½ë‹ˆë‹¤:
>
> **1) ìžê¸°ì†Œê°œ**  
> - **ê°œì„  ì‚¬í•­**: ìžê¸°ì†Œê°œ ë¶€ë¶„ì—ì„œâ€¦

#### Baseline Fine-Tuning + Curriculum Learning  
![Eval loss](./images/curriculum_eval_loss.png)  
| Configuration                     | Resume KoSimCSE Score | Self-Intro KoSimCSE Score |  
|----------------------------------|----------------------|--------------------------|  
| Baseline Fine-Tuning             | 0.8482               | 0.8501                   |  
| Baseline Fine-Tuning + Curriculum Learning | 0.8491     | 0.8276                   |

#### Sweep Results & Hyperparameter Settings
The key hyperparameters selected based on the Weights & Biases sweep results are as follows:

| Parameter                   | Value                          |
|----------------------------|-------------------------------|
| batch_size                 | 2                             |
| gradient_accumulation_steps | 4                             |
| learning_rate              | 4.869e-4                      |
| lora_r                    | 16                            |
| num_train_epochs          | 5                             |
| prompt_version            | "v2_explicit_guidelines_ko_improved" |

For detailed sweep results, please visit the [W&B project dashboard](https://wandb.ai/codream00-sungkyunkwan-university/resume_eval_ko_sweep_v4/sweeps/mszinhca?nw=nwusercodream00).

![Sweep eval loss](./images/sweep_eval_loss.png)
![Sweep result](./images/sweep_result.png)


#### User Context Injection via Pseudo-label
![pseudo-label eval graph](./images/pseudo-label_eval.png)
| Configuration (Training Strategy)              | Resume Similarity | Self-Intro Similarity |
|-----------------------------------------------|-------------------|------------------------|
| Basic                  | 0.7899            | 0.8080                 |
| Trained with Pseudo-label     | 0.8482            | 0.8501                 |

| Configuration (Training + Inference)       | Resume Similarity | Self-Intro Similarity |
|--------------------------------------------|-------------------|------------------------|
| Trained with Pseudo-label â€“ Inference: Off   | 0.8260        | 0.8302             |
| Trained with Pseudo-label â€“ Inference: On    | 0.8482            | 0.8501                 |

##### Pseudo-label KoSIMCSE
| User Context                               | Not Included      | Included |
|--------------------------------------------|-------------------|------------------------|
| Keyword Label   | 0.8136            | 0.8255             |
| Job Label       | 0.8186            | 0.8203                 |
| Resume Label    | 0.8214            | 0.8372                 |

**Note on Evaluation Metrics**  
- **Keyword Label** score is based on the **Self-Introduction Similarity**.  
- **Job Label** score is calculated as the **average of Resume Similarity and Self-Introduction Similarity**.  
- **Resume Label** score is based solely on **Resume Similarity**.

##### Reference
> ì •ê°œë°œì˜ ì´ë ¥ì„œëŠ” í¬ì§€ì…˜ì— ìš”êµ¬í•˜ëŠ” ê¸°ìˆ ê³¼ ê²½í—˜ì´ ìž˜ ë¶€í•©í•˜ë©°, íŠ¹ížˆ C# ë° .NET Coreë¥¼ í™œìš©í•œ ê²½í—˜ê³¼ SQL Serverì— ëŒ€í•œ ê¸°ë³¸ì  ì´í•´ê°€ ê°•ì¡°ë˜ì–´ ìžˆìŠµë‹ˆë‹¤. ê²Œìž„ ê°œë°œ ë™ì•„ë¦¬ì—ì„œì˜ íŒ€ í”„ë¡œì íŠ¸ ì°¸ì—¬ëŠ” ê²½í—˜ì„ ë”í•´ì£¼ë©°, RabbitMQ ë° Azure App Serviceì™€ ê°™ì€ ê´€ë ¨ íˆ´ì— ëŒ€í•œ ê²½í—˜ë„ ê¸ì •ì ì¸ ìš”ì†Œìž…ë‹ˆë‹¤. ìžê²©ìš”ê±´ì„ ì¶©ì¡±í•˜ê³ , ìš°ëŒ€ì‚¬í•­ì—ì„œë„ RabbitMQ ì‚¬ìš© ê²½í—˜ì´ ìžˆì–´ ë†’ì€ ì ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.

##### Trained with Pseudo-label - User Context Not Included
> ì •ê°œë°œì˜ ì´ë ¥ì„œëŠ” ì‹ ìž… ê°œë°œìžë¡œì„œì˜ ê¸°ë³¸ì ì¸ ìžê²©ìš”ê±´ì„ ì¶©ì¡±í•˜ê³  ìžˆìœ¼ë©°, ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ìžˆì–´ ê¸ì •ì ìž…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ê²½ë ¥ì´ ë¶€ì¡±í•˜ê³ , ê¸°ìˆ ì  ê¹Šì´ê°€ ë¶€ì¡±í•œ ì ì´ ì•„ì‰½ìŠµë‹ˆë‹¤. íŠ¹ížˆ, ì§€ì›í•˜ëŠ” í¬ì§€ì…˜ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê²½í—˜ì´ ë¶€ì¡±í•˜ì—¬ ê°ì ì´ ìžˆì—ˆìŠµë‹ˆë‹¤.

##### Trained with Pseudo-label - User Context included
> ì •ê°œë°œ ì§€ì›ìžì˜ ì´ë ¥ì„œëŠ” í¬ì§€ì…˜ì— ì í•©í•œ ê¸°ìˆ ê³¼ ê²½í—˜ì„ ìž˜ ë³´ì—¬ì£¼ê³  ìžˆìŠµë‹ˆë‹¤. C# ë°.NET Core, SQL Server, RabbitMQ, Azure App Service ë“± ìš”êµ¬ë˜ëŠ” ê¸°ìˆ  ìŠ¤íƒì„ ëª¨ë‘ ê°–ì¶”ê³  ìžˆìœ¼ë©°, ê²Œìž„ ê°œë°œ ë™ì•„ë¦¬ì—ì„œì˜ ê²½í—˜ì€ íŠ¹ížˆ ê¸ì •ì ìž…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, ê²½ë ¥ì´ ì‹ ìž… ìˆ˜ì¤€ìœ¼ë¡œ ì œí•œì ì´ë©°, ìš°ëŒ€ì‚¬í•­ ì¤‘ RabbitMQ ì‚¬ìš© ê²½í—˜ì´ ê¸°ë³¸ì ì¸ ìˆ˜ì¤€ì— ê·¸ì¹˜ê³  ìžˆì–´ ê°ì ì´ ìžˆì—ˆìŠµë‹ˆë‹¤. ì „ë°˜ì ìœ¼ë¡œ ì´ë ¥ì„œëŠ” ë§¤ìš° ìš°ìˆ˜í•˜ë©°, í¬ì§€ì…˜ì— ì í•©í•œ ì¸ìž¬ë¡œ í‰ê°€ë©ë‹ˆë‹¤.

### Prompt Learning
#### Hard Prompt
##### Hard Prompt A
```json
[
  {"role": "system", "content": "You are an assistant that evaluates resume documents."},
  {"role": "user", "content": "[Job-Post]\n...\n[Resume]\n...\n[Keywords]\n...\n[Self-Introduction]\n..."},
  {"role": "assistant", "content": "[eval_resume]: ...\n[eval_selfintro]: ...\n[summary]: ..."}
]
```
- Provides basic system role and separates sections with labels. Output is structured with labels, but no explicit evaluation guidelines are given.

##### Hard Prompt B
```json
[
  {"role": "system", "content": "Evaluate resumes."},
  {"role": "user", "content": "Job: ...\nResume: ...\nKeywords: ...\nSelf-Introduction: ..."},
  {"role": "assistant", "content": "...eval_resume text...\n...eval_selfintro text...\n...summary text..."}
]
```
- A minimal and concise format suitable for fast testing. It lacks output labels and guidance, which may reduce consistency.

##### Hard Prompt C
```json
[
  {"role": "system", "content": "ë‹¹ì‹ ì€ ìˆ™ë ¨ëœ ì¸ì‚¬(HR) ì–´ì‹œìŠ¤í„´íŠ¸ìž…ë‹ˆë‹¤. ... (í‰ê°€ ì§€ì¹¨ í¬í•¨)"},
  {"role": "user", "content": "[ì§ë¬´ ê³µê³ ] ...\n[ì´ë ¥ì„œ] ...\n[í‚¤ì›Œë“œ] ...\n[ìžê¸°ì†Œê°œì„œ] ...\n**í‰ê°€ ì§€ì¹¨** ..."},
  {"role": "assistant", "content": "[summary]: ...\n[eval_resume]: ...\n[eval_selfintro]: ..."}
]
```
- Includes explicit evaluation guidelines in Korean, making it the most structured and informative prompt. Helps maintain consistent quality and format across outputs.

##### Prompt KoSimCSE results
| Prompts              | Resume Similarity | Self-Intro Similarity |
|-----------------------------------------------|-------------------|------------------------|
| Hard Prompt A       | 0.8281            | 0.8237                 |
| Hard Prompt B       | 0.8174            | 0.8259                 |
| Hard Prompt C       | 0.8330            | 0.8359                 |
| Soft Prompt         | 0.8682            | 0.8598                 |

### Experimental Results: KoSimCSE Similarity by Training Strategy

| Configuration (Training Strategy)              | Resume Similarity | Self-Intro Similarity |
|-----------------------------------------------|-------------------|------------------------|
| Trained with Basic Prompt                  | 0.7899            | 0.8080                 |
| Trained with Explicit Guideline Prompt     | 0.8482            | 0.8501                 |
| Trained with Explicit Guideline + Curriculum | 0.8491            | 0.8276                 |
| + Soft Prompt | 0.8682             | 0.8598                 |

> All models share the same base (EXAONE-3.5), and the similarity scores represent average values measured by KoSimCSE.

